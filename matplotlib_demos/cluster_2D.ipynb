{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c40c50c",
   "metadata": {},
   "source": [
    "# Clustering in two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ad142df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "from functions import gen_domains2D, plot_domains2D, confidence_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e4d26d",
   "metadata": {},
   "source": [
    "This is an introduction to some of the considerations needed for data clustering in higher dimensions and an example of using Gaussian mixture models to predict the best cluster for unknown data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752f0108",
   "metadata": {},
   "source": [
    "## Normal distribution in higher dimensions\n",
    "* In one dimension a normal distrbution is described by $\\exp(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2})$, where $\\mu$ is the mean, $\\sigma$ is the standard deviation and $\\sigma^2$ is the variance\n",
    "* In higher dimensions this becomes $\\exp(-\\frac{1}{2}(X-M)^T \\Sigma (X-M))$, where $X$ and $M$ are $n$-dimensional vectors and $\\Sigma$ is the $n \\times n$ covariance matrix\n",
    "* The diagonal elements of $\\Sigma$ contain the variances of each dimension and the off-diagonal elements are the covariance between two dimensions\n",
    "* In 2D: $$\\Sigma = \\begin{pmatrix} \\sigma_x^2 & \\mathrm{cov}(x, y) \\\\ \\mathrm{cov}(x, y) & \\sigma_y^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma_x^2 & \\rho\\sigma_x\\sigma_y \\\\ \\rho\\sigma_x\\sigma_y & \\sigma_y^2 \\end{pmatrix}$$ where $\\rho$ is the Pearson correlation coefficient\n",
    "* This is demonstrated by the widget below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e51d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd77f7030ab4058a863986860b60680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from covariance_matrix import covariance_matrix\n",
    "fig, sliders = covariance_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7885f",
   "metadata": {},
   "source": [
    "## The advantages of higher dimensions\n",
    "* The example below shows a sample of three materials with two measurements (which could be _e.g._ surface potential and tapping phase)\n",
    "* Comparing the scatter plot to the histograms shows the advantage of higher dimensions:\n",
    "  * Clusters that are overlapping in 1D may be distinct in 2D or higher\n",
    "  * Distances in 1D are given by $x-x_0$ but in 2D are given by $\\sqrt{(x-x_0)^2 + (y-y_0)^2}$ (and so on for higher dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97343ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b21be23889461782c2bd665dd7b9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, materials = gen_domains2D(\n",
    "    populations=(1, 3, 1),\n",
    "    means=((-50, 101), (0, 100), (50, 99)),\n",
    "    stds=((10, 1/5), (10, 1/5), (10, 1/5)),\n",
    "    pearsons=(0, 0.4, 0)\n",
    ");\n",
    "plot_domains2D(images, materials);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c884ea5",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "* Clustering looks at distances between points to judge their similarity\n",
    "* But often, different observations have different units and variances so it's not fair to equate them\n",
    "* As an example, if I plot the scatter data from above, but use an equal scaling, the $A$ parameter completely dominates all the point-to-point distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a405df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da43b94ff2fc4023b5e3d1bd0686fe8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.scatter(*images.reshape(2, -1), alpha=0.05, s=1)\n",
    "ax.set(\n",
    "    xlabel='$A$',\n",
    "    ylabel='$B$',\n",
    "    aspect='equal'\n",
    ")\n",
    "for side in ('top', 'right'):\n",
    "    ax.spines[side].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af56ee",
   "metadata": {},
   "source": [
    "* We don't want to assume that either parameter is more important so we use Mahalanobis normalisation\n",
    "* Each dimension is normalised to its mean, $\\mu$, and standard deviation, $\\sigma$, as $\\frac{data - \\mu(data)}{\\sigma(data)}$\n",
    "* This means every observation has unit variance and zero mean\n",
    "* It also means the data is made dimensionless, which removes complications of different units\n",
    "* The scatter plot of Mahalanobis normalised data below shows that neither $A$ nor $B$ is given unfair precendence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f10900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a7e55f3951497088430ed3464a98d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mahalanobis = (\n",
    "    (images - images.mean(axis=(1, 2))[:, np.newaxis, np.newaxis])\n",
    "    / images.std(axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "ax.scatter(*mahalanobis.reshape(2, -1), s=1)\n",
    "ax.set(\n",
    "    xlabel=r'$\\frac{A-{\\mu}(A)}{{\\sigma}(A)}$',\n",
    "    ylabel=r'$\\frac{B-{\\mu}(B)}{{\\sigma}(B)}$',\n",
    "    aspect='equal'\n",
    ")\n",
    "ax.grid()\n",
    "for side in ('top', 'right'):\n",
    "    ax.spines[side].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6191f9",
   "metadata": {},
   "source": [
    "## Using clustering to classify new data\n",
    "* One of the main advantages of data clustering is that it can be used to automatically identify new data as belonging to a particular cluster\n",
    "* To show this I'm splitting the image above into two halves, top and bottom\n",
    "* I'll train the clustering on the top, and test it on the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d2ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_top = images[:, :64]\n",
    "materials_top = [mat[:64] for mat in materials]\n",
    "images_bottom = images[:, 64:]\n",
    "materials_bottom = [mat[64:] for mat in materials]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c496c",
   "metadata": {},
   "source": [
    "### Training phase\n",
    "* I start by normalising the data from the top half of the image\n",
    "* This is displayed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a963c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb57553693d4dc98982ac3c968fa5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_top = (\n",
    "    (images_top - images_top.mean(axis=(1, 2))[:, np.newaxis, np.newaxis])\n",
    "    / images_top.std(axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    ")\n",
    "plot_domains2D(norm_top, materials_top);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6d237",
   "metadata": {},
   "source": [
    "* It has three materials and so should be fit best by three clusters\n",
    "* I train a three-component Gaussian mixture model (GMM) on the data then calculate the probability each point belongs to a particular cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7626d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3).fit(norm_top.reshape(2, -1).T)\n",
    "labels_top = gmm.predict(norm_top.reshape(2, -1).T)\n",
    "probs_top = gmm.predict_proba(norm_top.reshape(2, -1).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d9674",
   "metadata": {},
   "source": [
    "* The scatter plot below shows the three identified clusters (ellipses show $1\\sigma$ away from the mean position)\n",
    "* The probability for each point being in the cluster is shown by the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f020881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564bb1da6889423b9b95d5700f3042c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 9))\n",
    "gs = plt.GridSpec(figure=fig, nrows=3, ncols=2, width_ratios=(2, 1))\n",
    "scatter_ax = fig.add_subplot(gs[:, 0])\n",
    "cluster_axes = [fig.add_subplot(gs[i, -1]) for i in range(3)]\n",
    "for i in range(3):\n",
    "    c = f'C{i+1}'\n",
    "    scatter_ax.scatter(*norm_top.reshape(2, -1)[:, labels_top==i], c=c, s=2)\n",
    "    confidence_ellipse(scatter_ax, gmm.means_[i], gmm.covariances_[i], fc=c, ls='--', lw=2, alpha=0.2)\n",
    "    cluster_axes[i].imshow(probs_top[:, i].reshape(images_top[0].shape))\n",
    "    cluster_axes[i].set_title(f'Cluster {i+1}', c=c)\n",
    "    cluster_axes[i].set_axis_off()\n",
    "for side in ('top', 'right'):\n",
    "    scatter_ax.spines[side].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42e67b",
   "metadata": {},
   "source": [
    "### New data\n",
    "* We now want to use the clusters found above from the top of the image, to assess the materials in the bottom of the image\n",
    "* To do this we need to apply the same normalisation as we did for the training data:\n",
    "  * For the training data we used Mahalanobis normalisation $\\frac{training - \\mu(training)}{\\sigma(training)}$\n",
    "  * For fairness we can't simply Mahalanobis normalise the testing data (it may have a different $\\mu$ and $\\sigma$)\n",
    "  * We need to do $\\frac{new data - \\mu(training)}{\\sigma(training)}$\n",
    "* The appropriately normalised new data from the bottom half of the image is shown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a9eac7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7eb21257a84bcbae1c391e5f32162d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_bottom = (\n",
    "    (images_bottom - images_top.mean(axis=(1, 2))[:, np.newaxis, np.newaxis])\n",
    "    / images_top.std(axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    ")\n",
    "plot_domains2D(norm_bottom, materials_bottom);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f02d98",
   "metadata": {},
   "source": [
    "* We still have the means and covariances of our Gaussian clusters found from the training data\n",
    "* We can use these to predict the likelihood that a point from the new data set belongs to each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f794aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52066eb9bdbd4334a2876fed43576052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs_bottom = gmm.predict_proba(norm_bottom.reshape(2, -1).T)\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "gs = plt.GridSpec(figure=fig, nrows=3, ncols=2, width_ratios=(2, 1))\n",
    "scatter_ax = fig.add_subplot(gs[:, 0])\n",
    "cluster_axes = [fig.add_subplot(gs[i, -1]) for i in range(3)]\n",
    "scatter_ax.scatter(*norm_bottom.reshape(2, -1), c='C0', s=2)\n",
    "for i in range(3):\n",
    "    c = f'C{i+1}'\n",
    "    confidence_ellipse(scatter_ax, gmm.means_[i], gmm.covariances_[i], fc=c, ls='--', lw=2, alpha=0.2)\n",
    "    cluster_axes[i].imshow(probs_bottom[:, i].reshape(images_bottom[0].shape))\n",
    "    cluster_axes[i].set_title(f'Cluster {i+1}', c=c)\n",
    "    cluster_axes[i].set_axis_off()\n",
    "for side in ('top', 'right'):\n",
    "    scatter_ax.spines[side].set_visible(False)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
